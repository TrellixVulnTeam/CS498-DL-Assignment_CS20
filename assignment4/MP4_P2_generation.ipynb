{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"MP4_P2_generation.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMT60ebJfVOZ","executionInfo":{"status":"ok","timestamp":1606072181193,"user_tz":360,"elapsed":1173,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"outputId":"d35daf3d-7c32-423a-875e-e0c6b3d296b3"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir(\"gdrive/My Drive/assignment4\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oqJptiaPfQ7Z"},"source":["# Generating Text with an RNN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLKJRvGGfQ7Z","executionInfo":{"status":"ok","timestamp":1606072184171,"user_tz":360,"elapsed":4137,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"outputId":"79e9b54a-f570-47f6-f538-289fa3c25a6f"},"source":["! pip install unidecode\n","import unidecode\n","import string\n","import random\n","import re\n","import time\n","import tqdm \n","import torch\n","import torch.nn as nn\n","\n","%matplotlib inline\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SRNjE_uffQ7Z","executionInfo":{"status":"ok","timestamp":1606072184175,"user_tz":360,"elapsed":4131,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["# from rnn.model import RNN\n","from rnn.helpers import time_since\n","from rnn.generate import generate"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"XyMVZownfQ7Z","executionInfo":{"status":"ok","timestamp":1606072184175,"user_tz":360,"elapsed":4126,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JdIumgPkfQ7Z"},"source":["## Data Processing\n","\n","The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJ6yswTEfQ7Z","executionInfo":{"status":"ok","timestamp":1606072184176,"user_tz":360,"elapsed":4120,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"outputId":"d9cb64e4-f10f-4c0a-82dc-7d1b6da9e981"},"source":["all_characters = string.printable\n","n_characters = len(all_characters)\n","\n","file_path = './shakespeare.txt'\n","file = unidecode.unidecode(open(file_path).read())\n","file_len = len(file)\n","print('file_len =', file_len)\n","\n","# we will leave the last 1/10th of text as test\n","split = int(0.9*file_len)\n","train_text = file[:split]\n","test_text = file[split:]\n","\n","print('train len: ', len(train_text))\n","print('test len: ', len(test_text))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["file_len = 4573338\n","train len:  4116004\n","test len:  457334\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gc8oraDlfQ7a","executionInfo":{"status":"ok","timestamp":1606072184176,"user_tz":360,"elapsed":4112,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"outputId":"e2dc32aa-f326-49ff-a3d3-42107ca78d8f"},"source":["chunk_len = 200\n","\n","def random_chunk(text):\n","    start_index = random.randint(0, len(text) - chunk_len)\n","    end_index = start_index + chunk_len + 1\n","    return text[start_index:end_index]\n","\n","print(random_chunk(train_text))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["on; let me hear\n","What you profess.\n","\n","FLORIZEL:\n","Do, and be witness to 't.\n","\n","POLIXENES:\n","And this my neighbour too?\n","\n","FLORIZEL:\n","And he, and more\n","Than he, and men, the earth, the heavens, and all:\n","That, were I\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZxLkW7wJfQ7a"},"source":["### Input and Target data"]},{"cell_type":"markdown","metadata":{"id":"kGyCKdeGfQ7a"},"source":["To make training samples out of the large string of text data, we will be splitting the text into chunks.\n","\n","Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."]},{"cell_type":"code","metadata":{"id":"J5kFJmDwfQ7a","executionInfo":{"status":"ok","timestamp":1606072184177,"user_tz":360,"elapsed":4108,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["# Turn string into list of longs\n","def char_tensor(string):\n","    tensor = torch.zeros(len(string), requires_grad=True).long()\n","    for c in range(len(string)):\n","        tensor[c] = all_characters.index(string[c])\n","    return tensor"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJ-DbHdTfQ7a"},"source":["The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"]},{"cell_type":"code","metadata":{"id":"rwiQWNZJfQ7a","executionInfo":{"status":"ok","timestamp":1606072184178,"user_tz":360,"elapsed":4104,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["def load_random_batch(text, chunk_len, batch_size):\n","    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n","    target = torch.zeros(batch_size, chunk_len).long().to(device)\n","    for i in range(batch_size):\n","        start_index = random.randint(0, len(text) - chunk_len - 1)\n","        end_index = start_index + chunk_len + 1\n","        chunk = text[start_index:end_index]\n","        input_data[i] = char_tensor(chunk[:-1])\n","        target[i] = char_tensor(chunk[1:])\n","    return input_data, target"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bNwXV6WfQ7a"},"source":["# Implement model\n","\n","Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n","\n","\n","You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n","\n","\n","**TODO:** Implement the model in RNN `rnn/model.py`"]},{"cell_type":"code","metadata":{"id":"YsfRslTbmh7v","executionInfo":{"status":"ok","timestamp":1606072184778,"user_tz":360,"elapsed":4700,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, model_type=\"rnn\", n_layers=1):\n","        super(RNN, self).__init__()\n","        \"\"\"\n","        Initialize the RNN model.\n","\n","        You should create:\n","        - An Embedding object which will learn a mapping from tensors\n","        of dimension input_size to embedding of dimension hidden_size.\n","        - Your RNN network which takes the embedding as input (use models\n","        in torch.nn). This network should have input size hidden_size and\n","        output size hidden_size.\n","        - A linear layer of dimension hidden_size x output_size which\n","        will predict output scores.\n","\n","        Inputs:\n","        - input_size: Dimension of individual element in input sequence to model\n","        - hidden_size: Hidden layer dimension of RNN model\n","        - output_size: Dimension of individual element in output sequence from model\n","        - model_type: RNN network type can be \"rnn\" (for basic rnn), \"gru\", or \"lstm\"\n","        - n_layers: number of layers in your RNN network\n","        \"\"\"\n","\n","        self.model_type = model_type\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","\n","        ####################################\n","        #          YOUR CODE HERE          #\n","        ####################################\n","\n","        self.input2hidden  = nn.Embedding(input_size, hidden_size)\n","        self.hidden2output = nn.Linear(hidden_size, output_size)\n","\n","        if model_type == \"rnn\":\n","            self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n","        elif model_type == \"lstm\":\n","            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n","        elif model_type == \"gru\":\n","            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n","\n","        ##########       END      ##########\n","\n","\n","\n","    def forward(self, input, hidden):\n","        \"\"\"\n","        Forward pass through RNN model. Use your Embedding object to create\n","        an embedded input to your RNN network. You should then use the\n","        linear layer to get an output of self.output_size.\n","\n","        Inputs:\n","        - input: the input data tensor to your model of dimension (batch_size)\n","        - hidden: the hidden state tensor of dimension (n_layers x batch_size x hidden_size)\n","\n","        Returns:\n","        - output: the output of your linear layer\n","        - hidden: the output of the RNN network before your linear layer (hidden state)\n","        \"\"\"\n","\n","        output = None\n","\n","        ####################################\n","        #          YOUR CODE HERE          #\n","        ####################################\n","        batch_size = input.shape[0]\n","        embedding = self.input2hidden(input).view(1, batch_size, -1)\n","        # required input size for nn.RNN: seq_len, batch, input_size\n","        output, hidden = self.rnn(embedding, hidden)\n","        output = self.hidden2output(output.view(1, batch_size, -1))\n","\n","        ##########       END      ##########\n","        return output, hidden\n","\n","    def init_hidden(self, batch_size, device=None): \n","        \"\"\"\n","        Initialize hidden states to all 0s during training.\n","\n","        Hidden states should be initilized to dimension (n_layers x batch_size x hidden_size)\n","\n","        Inputs:\n","        - batch_size: batch size\n","\n","        Returns:\n","        - hidden: initialized hidden values for input to forward function\n","        \"\"\"\n","\n","        hidden = None\n","\n","        ####################################\n","        #          YOUR CODE HERE          #\n","        ####################################\n","\n","        if self.model_type == \"lstm\":\n","            hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device),\n","                    torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device))\n","        else:\n","            hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n","\n","        ##########       END      ##########\n","\n","        return hidden"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s9MGO5bcfQ7a"},"source":["# Evaluating\n","\n","To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n","\n","\n","Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n","\n","You may check different temperature values yourself, but we have provided a default which should work well."]},{"cell_type":"code","metadata":{"id":"jJnWIFIsfQ7a","executionInfo":{"status":"ok","timestamp":1606072184778,"user_tz":360,"elapsed":4695,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n","    hidden = rnn.init_hidden(1, device=device)\n","    prime_input = char_tensor(prime_str)\n","    predicted = prime_str\n","\n","    # Use priming string to \"build up\" hidden state\n","    for p in range(len(prime_str) - 1):\n","        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n","    inp = prime_input[-1]\n","    \n","    for p in range(predict_len):\n","        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n","        \n","        # Sample from the network as a multinomial distribution\n","        output_dist = output.data.view(-1).div(temperature).exp()\n","        top_i = torch.multinomial(output_dist, 1)[0]\n","        \n","        # Add predicted character to string and use as next input\n","        predicted_char = all_characters[top_i]\n","        predicted += predicted_char\n","        inp = char_tensor(predicted_char)\n","\n","    return predicted"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZVyCeFbgfQ7a"},"source":["# Train RNN"]},{"cell_type":"code","metadata":{"id":"Q8WygfqpfQ7a","executionInfo":{"status":"ok","timestamp":1606072184779,"user_tz":360,"elapsed":4692,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["batch_size = 100\n","n_epochs = 2500\n","hidden_size = 150\n","n_layers = 3\n","learning_rate = 0.01\n","model_type = 'lstm'\n","print_every = 50\n","plot_every = 50\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkdfWWZufQ7a","executionInfo":{"status":"ok","timestamp":1606072184780,"user_tz":360,"elapsed":4690,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["def eval_test(rnn, inp, target):\n","    with torch.no_grad():\n","        hidden = rnn.init_hidden(batch_size, device=device)\n","        loss = 0\n","        for c in range(chunk_len):\n","            output, hidden = rnn(inp[:,c], hidden)\n","            loss += criterion(output.view(batch_size, -1), target[:,c])\n","    \n","    return loss.data.item() / chunk_len"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dtz-En7EfQ7a"},"source":["### Train function\n","\n","**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"]},{"cell_type":"code","metadata":{"id":"Gl0y5VKZfQ7a","executionInfo":{"status":"ok","timestamp":1606072184780,"user_tz":360,"elapsed":4687,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["def train(rnn, input, target, optimizer, criterion):\n","    \"\"\"\n","    Inputs:\n","    - rnn: model\n","    - input: input character data tensor of shape (batch_size, chunk_len)\n","    - target: target character data tensor of shape (batch_size, chunk_len)\n","    - optimizer: rnn model optimizer\n","    - criterion: loss function\n","    \n","    Returns:\n","    - loss: computed loss value as python float\n","    \"\"\"\n","    loss = 0\n","    \n","    ####################################\n","    #          YOUR CODE HERE          #\n","    ####################################\n","    batch_size = input.shape[0]\n","    optimizer.zero_grad()\n","    hidden = rnn.init_hidden(batch_size, device = device)\n","    \n","    for i in range(chunk_len):\n","        pred, hidden = rnn(input[:, i], hidden)\n","        loss += criterion(pred.view(batch_size, -1), target[:, i])\n","    loss /= chunk_len\n","    loss.backward()\n","    optimizer.step()\n","        \n","    ##########       END      ##########\n","\n","    return loss \n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHcYzh8AfQ7a","executionInfo":{"status":"ok","timestamp":1606074473458,"user_tz":360,"elapsed":2293351,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"outputId":"153a0c80-6cd2-49c8-e466-5a0b686d7042"},"source":["rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n","rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","all_losses = []\n","test_losses = []\n","loss_avg = 0\n","test_loss_avg = 0\n","\n","\n","print(\"Training for %d epochs...\" % n_epochs)\n","for epoch in range(1, n_epochs + 1):\n","    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n","    loss_avg += loss\n","    \n","    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n","    test_loss_avg += test_loss\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n","        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        test_losses.append(test_loss_avg / plot_every)\n","        loss_avg = 0\n","        test_loss_avg = 0"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Training for 2500 epochs...\n","[0m 45s (50 2%) train loss: 2.8409, test_loss: 2.8242]\n","Wh dare tl h ousrinod,t lal m ns treiser nlhegsrs se\n","\n","\n","DSRNRCLI:\n","'on todr c v s,,e;\n","\n","GSOUAS:\n","Hor  bers \n","\n","[1m 31s (100 4%) train loss: 2.3806, test_loss: 2.3799]\n","Wher hind pat woce thann der, hroterheas ke lilge then bere witoe,, bheat rosak Cis?\n","\n","TORELBONDAS:\n","Tor \n","\n","[2m 16s (150 6%) train loss: 2.0864, test_loss: 2.1445]\n","Whe prishice rettard the,\n","As\n","Thure ene me srowpy.\n","\n","BHALDALUS ODONDANUS:\n","Anging highe onow be the thaf! \n","\n","[3m 2s (200 8%) train loss: 1.9622, test_loss: 1.9877]\n","Whold dad, thold the spering scerll! theo't\n","The fregred I aud?\n","\n","PROSLES:\n","And the destighar: sell nom d \n","\n","[3m 47s (250 10%) train loss: 1.8296, test_loss: 1.9106]\n","When me your to conse Counk I him they for the word,\n","Onany, end swear hath be as to rasch.\n","\n","PANDARUS:\n"," \n","\n","[4m 33s (300 12%) train loss: 1.7503, test_loss: 1.8475]\n","Whis his our her in a man the this.\n","\n","DUKE QUBET:\n","All isle of he come with.\n","\n","DUKE MONTAREY:\n","Well the po \n","\n","[5m 18s (350 14%) train loss: 1.6886, test_loss: 1.7899]\n","Where you mailand,\n","For sors to sleesssed I am says, and his pruce,\n","To have can woid to seest shesswelv \n","\n","[6m 3s (400 16%) train loss: 1.6406, test_loss: 1.7453]\n","Wheth your encurame him. For where\n","right and was his more that it set have is and counself,\n","That he be \n","\n","[6m 48s (450 18%) train loss: 1.5949, test_loss: 1.7115]\n","Whink then worbet some this own the moderefos\n","Maite had well we not slow him as oads,\n","And must not as  \n","\n","[7m 33s (500 20%) train loss: 1.5845, test_loss: 1.6831]\n","Where your siless of and to still to ryon\n","thou was your grown you not me:\n","I'll seither heaver to me th \n","\n","[8m 20s (550 22%) train loss: 1.5642, test_loss: 1.6754]\n","Whan she you will go up the other\n","The wratchful good Tower man of this way\n","And world him a good:\n","You b \n","\n","[9m 6s (600 24%) train loss: 1.5499, test_loss: 1.6291]\n","When, I have indeed, and say you and make his more\n","Than he sound to the dead and prophiet age seek peq \n","\n","[9m 53s (650 26%) train loss: 1.5369, test_loss: 1.6543]\n","What shall with gradater him. Head.\n","\n","CLEOPATRA:\n","Prove of cellent striving I have gives? Lettender.\n","\n","FE \n","\n","[10m 39s (700 28%) train loss: 1.4973, test_loss: 1.6401]\n","Whom it here would be their princed in them\n","Wherefound lovely followers, tell her clesare that titling \n","\n","[11m 25s (750 30%) train loss: 1.5096, test_loss: 1.5975]\n","Where thou was slave;\n","Almentious, the his for from my Lord\n","apvice with a gentlemen intender thee\n","The w \n","\n","[12m 12s (800 32%) train loss: 1.4843, test_loss: 1.6062]\n","Whot say your house stand your last Shall\n","And leave my court and living; think\n","with your such a preces \n","\n","[12m 58s (850 34%) train loss: 1.4741, test_loss: 1.6038]\n","Who wild,\n","And his wear you have you and received of the lady and man,\n","Most do not gild, sir, honest, s \n","\n","[13m 45s (900 36%) train loss: 1.4867, test_loss: 1.5962]\n","When I say his breathes!\n","\n","LEONATO:\n","Where you hold you men thou hadst her;\n","Perconours present your day, \n","\n","[14m 32s (950 38%) train loss: 1.4397, test_loss: 1.5692]\n","Why, give you, have nothing wither.\n","\n","HORATIO:\n","O, die so eyes shall gave!\n","\n","ANTIPHOLUS OF SYRACUSE:\n","Ha!\n"," \n","\n","[15m 18s (1000 40%) train loss: 1.4696, test_loss: 1.5515]\n","Who moved, let such a scorn shall of all the earth with thee,\n","For thou shall effection thy pray hath b \n","\n","[16m 5s (1050 42%) train loss: 1.4444, test_loss: 1.5495]\n","When one appoll me to bear them with you\n","And what true of the words chis invised,\n","Than the house and b \n","\n","[16m 50s (1100 44%) train loss: 1.4475, test_loss: 1.5507]\n","When but love no king of some heart.\n","\n","MARK ANTONY:\n","Where shall the heaven of any safeable my mother.\n","C \n","\n","[17m 36s (1150 46%) train loss: 1.4300, test_loss: 1.5642]\n","Where sing of present that I say, he stand\n","Thou wouldst hear your stretted of the bours,\n","And he and to \n","\n","[18m 22s (1200 48%) train loss: 1.4169, test_loss: 1.5760]\n","What minist staining shall soldiers.\n","\n","DUKE VINCENTIO:\n","See, if he bares, as I'll make them are meet the \n","\n","[19m 7s (1250 50%) train loss: 1.4199, test_loss: 1.5398]\n","What hang it far it, I will render grace\n","To the purse with the sun o'ersoit his bones.\n","The impall and  \n","\n","[19m 53s (1300 52%) train loss: 1.4207, test_loss: 1.5347]\n","What I am, and show so much see in chape to compation\n","Than welcome, my fair Marchand Burguard with me, \n","\n","[20m 39s (1350 54%) train loss: 1.3831, test_loss: 1.5698]\n","Where dear me to his losses with the French; and the conchorm;\n","And that you must make you that thou sh \n","\n","[21m 25s (1400 56%) train loss: 1.4115, test_loss: 1.5327]\n","Whese traitor, and here, sir, what which dispose\n","To know the list to his officels of strong\n","The change \n","\n","[22m 10s (1450 57%) train loss: 1.3968, test_loss: 1.5513]\n","Why drowned more in the rest your places in my love to a word.\n","\n","BALTHASSAO:\n","So much persuled these dis \n","\n","[22m 56s (1500 60%) train loss: 1.3991, test_loss: 1.5489]\n","Why a trashing for the formest to heaven:\n","Her ancurpess betray and was mad with change.\n","\n","NORDALIND:\n","No \n","\n","[23m 41s (1550 62%) train loss: 1.4009, test_loss: 1.5783]\n","Why, thou shalt would I have gotty?\n","\n","HAMLET:\n","I cast, that's will shall truth!\n","\n","GLOUCESTER:\n","As I'll per \n","\n","[24m 27s (1600 64%) train loss: 1.3900, test_loss: 1.5012]\n","Whider that loving this countent. Come\n","A lest and sudden is to the service\n","To gain to show the provost \n","\n","[25m 13s (1650 66%) train loss: 1.3718, test_loss: 1.5096]\n","Who shall go on the proudest for me very favour grown\n","In Brutus is with all in the honourano's poor so \n","\n","[25m 58s (1700 68%) train loss: 1.3978, test_loss: 1.5186]\n","Why whis but the treasy met to some to the rather and\n","rather commrage of her command of himself.\n","\n","NORT \n","\n","[26m 44s (1750 70%) train loss: 1.3760, test_loss: 1.5041]\n","What delivers the king; which bound then I shall not not she hath\n","That sent to the amkard of any of hi \n","\n","[27m 29s (1800 72%) train loss: 1.3807, test_loss: 1.5121]\n","Whream and so much depend outward it;\n","Who say I slander more any fellows, as much:\n","That fear, nor one  \n","\n","[28m 15s (1850 74%) train loss: 1.3602, test_loss: 1.4910]\n","Where I hear, come you.\n","\n","GREMIO:\n","I know no souls here mean.\n","\n","MISTRESS PAGE:\n","A present met it down, or  \n","\n","[29m 0s (1900 76%) train loss: 1.3531, test_loss: 1.5141]\n","Who say you that hath show to our son left\n","Than the courts, and now I finds it to say and battle\n","In hi \n","\n","[29m 45s (1950 78%) train loss: 1.3723, test_loss: 1.4715]\n","Which I prince? do not need before you with the words\n","And given him yet a truth chaped heart my hands. \n","\n","[30m 31s (2000 80%) train loss: 1.3809, test_loss: 1.4892]\n","What devil, the duke lies.\n","\n","ANTIPHOLUS OF EPHESUS:\n","And Trom Slenkard is written water better matter.\n","\n"," \n","\n","[31m 16s (2050 82%) train loss: 1.3643, test_loss: 1.4785]\n","Where be a queen; which I say I have been here,\n","That white all the rike of such noble princes are\n","And  \n","\n","[32m 2s (2100 84%) train loss: 1.3541, test_loss: 1.4984]\n","Where is this solicate swords,\n","With that seem, the weaks of answer that is to\n","catenet against you with \n","\n","[32m 47s (2150 86%) train loss: 1.3655, test_loss: 1.5118]\n","Where are the gods in a duke of his hand; and good farther.\n","The wretches I thus coming for his eyes, b \n","\n","[33m 33s (2200 88%) train loss: 1.3281, test_loss: 1.4735]\n","Which the counterfeit to hear your power;\n","The evil is the world from the dibination,\n","I know her dry wi \n","\n","[34m 18s (2250 90%) train loss: 1.3318, test_loss: 1.4886]\n","What thou strike him as suspect our the heir\n","What was man to the jointality.\n","\n","ESCALUS:\n","And I will so m \n","\n","[35m 3s (2300 92%) train loss: 1.3358, test_loss: 1.4868]\n","Which this spirit would be straight or infect of self fast\n","To rare a rooters of my daughter;\n","And would \n","\n","[35m 48s (2350 94%) train loss: 1.3285, test_loss: 1.4649]\n","Why think, good friar, break to say thou, good Must\n","See some welcome to his own as woman's humour's\n","ch \n","\n","[36m 34s (2400 96%) train loss: 1.3325, test_loss: 1.4699]\n","Where dead, to say to me with thee:\n","Beauty's hate, and every king he your father to\n","not commanded and  \n","\n","[37m 19s (2450 98%) train loss: 1.3139, test_loss: 1.5182]\n","What past ambined conveasure,\n","Her occasion to me, says I am proclass.\n","\n","GLOUCESTER:\n","And thou shall be a \n","\n","[38m 4s (2500 100%) train loss: 1.3516, test_loss: 1.4549]\n","Where is the waterised\n","And the Eveal curse mad has took me the dood.\n","\n","LUCIUS:\n","I will die, the gods in  \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SXOsdAfrfQ7a","executionInfo":{"status":"ok","timestamp":1606074473460,"user_tz":360,"elapsed":2293347,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":["# save network\n","# torch.save(classifier.state_dict(), './rnn_generator.pth')"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LPOc9s_5fQ7a"},"source":["# Plot the Training and Test Losses"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"YuSsttHEfQ7a","executionInfo":{"status":"ok","timestamp":1606074473461,"user_tz":360,"elapsed":2293345,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"outputId":"c4ad0000-6878-4561-f082-77c70ab64e29"},"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","plt.figure()\n","plt.plot(all_losses)\n","plt.plot(test_losses, color='r')"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fe28310f780>]"]},"metadata":{"tags":[]},"execution_count":16},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"4RGWA7xcfQ7a"},"source":["# Evaluate text generation\n","\n","Check what the outputted text looks like"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqUXfeu_fQ7a","executionInfo":{"status":"ok","timestamp":1606074474612,"user_tz":360,"elapsed":2294492,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}},"outputId":"8a70de99-b847-415b-c53f-ad11aa3c5ac7"},"source":["print(evaluate(rnn, prime_str='Th', predict_len=1000))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["The king that she was a bra's beauty in the servants!\n","What, is my brothers, prepare?\n","\n","AUTOLYCUS:\n","Well, which I have took the action: sleep, shall I care\n","such unmoneyly strike and precice of the\n","estible, no whein are at my messenger; let him\n","there is our tongue. No monsterable: I have,\n","therefore shall come to this spirit to blastine\n","I repair the cloud, and after that our own.\n","\n","ORLANDO:\n","Such, sir, you master and proved the basking: I\n","can beat the servants:' it were too sword.\n","\n","PRINCE EDWALD:\n","O, I think I detest you out of this the dead,\n","Which shall I have as do you do I\n","Have dinent to serve our cursely with\n","here and back of the father sitched then.\n","\n","COUNTESS:\n","What is mine a horses at your best brave.\n","\n","GOWER:\n","Point, who saw the base day?\n","\n","OTHELLO:\n","And part, I think I am glad all the newly to fierce.\n","\n","RIVERS:\n","A man be so stood by me; then profess'd with a revolt\n","Whom what dies for from the baamony,\n","Even and the like comfort and from what eitherto\n","Which be see born. How long Page, sweet sweet\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lZ4ElU4TfQ7a"},"source":["# Hyperparameter Tuning\n","\n","Some things you should try to improve your network performance are:\n","- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n","- Try adding 1 or two more layers\n","- Increase the hidden layer size\n","- Changing the learning rate\n","\n","**TODO:** Try changing the RNN type and hyperparameters. Record your results."]},{"cell_type":"code","metadata":{"id":"dkQNu3hxfQ7a","executionInfo":{"status":"ok","timestamp":1606074474613,"user_tz":360,"elapsed":2294490,"user":{"displayName":"Yutong Xie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXdeoCfgkQhGM6Y9yxqYJammXB7UaVouj7KqsW=s64","userId":"16538548856605091169"}}},"source":[""],"execution_count":17,"outputs":[]}]}